---
title: 就业技术加强(07)-微服务扩展讲义
tags:
  - 笔记
  - 微服务
  - Spring Cloud
  - Zuul
  - 限流
  - Nginx限流
  - Gateway限流
  - 分布式日志系统
  - GrayLog
  - 分布式链路追踪
  - Zipkin
  - skywalking 
categories:
  - 就业技术加强
date: 2021-01-31 22:54:18
---



## 0、课程目标

目标1：能够说出Spring Cloud微服务组件

目标2：能够使用网关Zuul

目标3：掌握Nginx限流与Gateway限流

目标4：使用分布式日志系统GrayLog

目标5：使用分布式链路追踪Zipkin

目标6：使用分布式链路追踪skywalking 

## 1、微服务

### 1.1、DUBBO

若以DUBBO作为分布式微服务架构的话，一般技术栈组合为： DUBBO + Spring Boot

### 1.2、Spring Cloud

#### 1.2.1、Spring Cloud

Spring Cloud 提供了一些可以让开发者快速构建分布式应用的组件，这些组件可以很好的在任何分布式环境下工作。

![image-20210107114634261](就业技术加强-07-微服务扩展-1.assets/image-20210107114634261.png)

> Gradle与Maven都是自动化构建工具，虽然目前来说Maven是行业标准，而Gradle是后起之秀，它抛弃了Maven中基于XML的繁琐配置，大大简化了构建代码的行数。

#### 1.2.2、组件回顾

- **注册中心Eureka**

注册中心可以说是微服务架构中的”通讯录“，它记录了服务和服务地址的映射关系，当服务需要调用其它服务时，就到这里找到服务的地址，进行调用。同类型的有Nacos、consul、zookeeper。

| 特性             | Eureka | Nacos | Consul | Zookeeper |
| ---------------- | ------ | ----- | ------ | --------- |
| 一致性协议       | AP     | AP+CP | CP     | CP        |
| 负载均衡策略     | ribbon | 权重  | fabio  |           |
| 雪崩             | 支持   | 支持  |        |           |
| Spring Cloud集成 | 支持   | 支持  | 支持   | 支持      |
| DUBBO集成        | 不支持 | 支持  | 支持   | 支持      |

> - 一致性(Consistency) 所有节点在同一时间具有相同的数据
> - 可用性(Availability) 保证每个请求不管成功或者失败都有响应；只要收到用户的请求，服务器就必须给出回应
> - 分区容错性(Partition tolerance) 系统中任意信息的丢失或失败不会影响系统的继续运作；跨区的服务器之间通信可能接收不到，但是不影响系统使用

Eureka的`AP` 如何解决Eureka集群服务器节点之间的数据一致性问题？

```
1、Eureka Server管理了全部的服务器列表
2、当 Eureka Server 收到客户端的注册、下线、心跳请求时，通过服务器列表向其余的服务器进行消息广播，如果广播失败则重试，直到任务过期后取消任务，此时这两台服务器之间数据会出现短暂的不一致。
3、如果网络恢复正常，收到了其它服务器广播的心跳任务，此时可能有二种情况
  1. 该实例已经自动过期，则重新进行注册；
  2. 数据冲突，出现不一致的情况，则需要发起同步请求，其实也就是重新注册一次，同时踢除老的实例。

总之，通过集群之间的消息广播可以实现数据的最终一致性。

```

![1591003628876](就业技术加强-07-微服务扩展-1.assets/1591003628876.png)

**服务注册**

- Eureka在应用启动时,主动去 Eureka Server 端注册

- 应用会开启一个30秒执行一次的定时任务，该任务会去监测自身的 IP 信息以及自身的配置信息是否发生改变，如果发生改变，则会重新发起注册，参数配置`eureka.client.instance-info-replication-interval-seconds=30`。

  ![1591003723375](就业技术加强-07-微服务扩展-1.assets/1591003723375.png)  

**服务下线**

客户端 首先会将自身的状态改为 DOWN，接着发送下线命令至 Eureka Server 请求下掉自己的服务。

![1591003771197](就业技术加强-07-微服务扩展-1.assets/1591003771197.png) 

**服务续约**

服务提供者在注册完服务之后，服务提供者会维护一个心跳用来持续告诉EurekaServer：“我还活着”，以防止EurekaServer的“剔除任务”，简称：“服务续约”，参数配置:`eureka.instance.leaseRenewalIntervalInSeconds`默认是30秒

![1591003804770](就业技术加强-07-微服务扩展-1.assets/1591003804770.png) 

**服务踢出**

服务提供者服务失效时间`eureka.instance.leaseExpirationDurationInSeconds`默认90秒，如果eureka Server在90秒没有收到续约则会把该服务踢出

![1591003846308](就业技术加强-07-微服务扩展-1.assets/1591003846308.png) 

**自我保护**

- 为了避免网络连接故障，服务未按时进行心跳续约时,Eureka Server 在运行期间会去统计心跳失败比例在 15 分钟之内是否低于 85%，如果低于 85%，Eureka Server 会将这些实例保护起来，不再踢出任何服务，当网络故障恢复后，Eureka Server自动退出自我保护模式

- 关闭保护机制配置:`eureka.server.enable-self-preservation=false` (**不推荐**)

  ![1591003902796](就业技术加强-07-微服务扩展-1.assets/1591003902796.png)  

- **负载均衡Ribbon**

Ribbon在我们实际开发场景单独使用的地方不多，都是与别的组件组合使用，比如Feign/Gateway

Ribbon作为后端负载均衡器，比Nginx更注重的是承担并发（意思就是说这些服务都要处理，只是有不同的策略进行处理）而不是请求分发，可以直接感知后台动态变化来指定分发策略。它一共提供了7种负载均衡策略：

![1594627941879](就业技术加强-07-微服务扩展-1.assets/1594627941879.png)

- **熔断器Hystrix**

分布式环境下，服务跟服务之间需要进行依赖，一个业务调用通常依赖多个基础服务。

例如：对于同步调用，当库存服务不可用，商品服务请求线程阻塞，如果这个时候有大批量的请求过来，最终会导致整个商品服务资源耗尽，导致商品服务也无法提供服务，并且这种不可用可能会继续影响上游服务，这种现象称为**雪崩**效应

![1591156095367](就业技术加强-07-微服务扩展-1.assets/1591156095367.png) 

Hystrix能够保证在一个微服务出问题的前提下，不会导致整个服务失败，避免级联故障，以提高分布式系统的弹性，即避免雪崩。

**雪崩常见的效应场景**

- 硬件故障：如服务器宕机，机房断电，光纤被挖断等
- 流量激增：如异常流量，促销活动等
- 程序BUG：如程序逻辑导致内存泄漏，JVM长时间FullGC等
- 同步等待：服务调用采用同步模式，同步等待造成的资源耗尽

**雪崩效应应对策略**

- 硬件故障：服务分布式部署

  ![1591156253192](就业技术加强-07-微服务扩展-1.assets/1591156253192.png)  

- 流量激增：服务扩容、限流

  ![1591156274182](就业技术加强-07-微服务扩展-1.assets/1591156274182.png)  

- 程序BUG：修改程序bug、及时释放资源

- 同步等待：资源隔离、MQ解耦，不可用服务调用快速失败进行降级处理

  ![1591156300296](就业技术加强-07-微服务扩展-1.assets/1591156300296.png)  

- **服务调用Feign**

**微服务 与 微服务 之间调用问题**

- 调用URL问题
- 请求参数问题
- 响应数据问题

**Feign存在的目的就是为了简化微服务之间的调用**，它可以把这些问题全部伪装成一个Feign的http客户端接口，默认集成RestTemplate。 

Feign可以把Rest的请求进行隐藏，伪装SpringMVC的Controller一样。你不用再自己拼接url，拼接参数等等操作，一切都交给Feign去做。(从而简化java语言http客户端远程调用问题) 

项目主页：https://github.com/OpenFeign/feign 

- **配置中心Config**

Spring Cloud Config: 用来管理配置信息文件，它从配置仓库获取所有的配置信息，而其他的服务从配置中心获取自己所需要的配置信息；配置，指的是服务配置中心，就是所有的服务配置都中心化。

**配置中心好处:** 

- 统一配置的增删改查
- 不同环境配置隔离（开发、测试、上线）
- 高性能、高可用性
- 请求量多、高并发

**国内知名配置中心框架:**

- 携程：Apollo，配置中心 <https://github.com/ctripcorp/apollo>
- 阿里：Nacos，它是注册中心和配置中心的结合体 <https://nacos.io/zh-cn/> 

**配置中心原理图:** 

![1555847177033](就业技术加强-07-微服务扩展-1.assets/1555847177033.png)

## 2、微服务网关

### 2.1、网关

服务**网关** = **过滤**器 + 服务发现 + **路由**转发 

- 过滤器：比如验签、权限校验、限流等等

- 服务发现：找到服务提供方

- 路由转发：接收一切外界请求，转发到后端的微服务上去

- 没有网关的架构

  ![1591264634415](就业技术加强-07-微服务扩展-1.assets/1591264634415.png)  

- 有网关的架构

  ![1591265023744](就业技术加强-07-微服务扩展-1.assets/1591265023744.png)  

  

### 2.2、Gateway

- Spring Cloud Gateway是Spring官网基于Spring 5.0、 Spring Boot 2.0、Project Reactor等技术开发的网关服务。
- Spring Cloud Gateway基于Filter链提供网关基本功能：安全、监控／埋点、限流等。
- Spring Cloud Gateway为微服务架构提供简单、有效且统一的API路由管理方式。
- Spring Cloud Gateway是替代Netflix Zuul的一套解决方案。

Spring Cloud Gateway组件的核心是一系列的过滤器，通过这些过滤器可以将客户端发送的请求转发（路由）到对应的微服务。 Spring Cloud Gateway是加在整个微服务最前沿的防火墙和代理器，隐藏微服务结点IP端口信息，从而加强安全保护。Spring Cloud Gateway本身也是一个微服务，需要注册到Eureka服务注册中心。

![1591338821942](就业技术加强-07-微服务扩展-1.assets/1591338821942.png)

在Gateway内部，先启动一个netty server（默认端口为8080）接受请求，然后通过Routes（每个Route由Predicate(等同于HandlerMapping)和Filter(等同于HandlerAdapter)）处理后通过Netty Client发给响应的微服务。

### 2.3、Zuul

#### 2.3.1、简介

Zuul是SpringCloud全家桶中的微服务API网关，本质上是一个Servlet，也是一个多线程阻塞模型。

多线程阻塞模型：服务端可以接受多个请求，但是每一个请求在结果响应之前都处于阻塞状态。

#### 2.3.2、应用说明

1）导入 `资料\演示工程\springcloud-demo` 

2）分别启动  `heima-eureka` `heima-user` `heima-zuul`

3）访问 http://localhost:10087/user/1

![image-20210107205203656](就业技术加强-07-微服务扩展-1.assets/image-20210107205203656.png)



#### 2.3.3、与Gateway区别

| 网关组件             | 基于框架           | 阻塞模型               | 容器   |
| -------------------- | ------------------ | ---------------------- | ------ |
| Zuul 1.x             | Servlet            | 多线程阻塞模型         | Tomcat |
| Spring Cloud Gateway | Spring5.+  WebFlux | 响应式的、非阻塞式模型 | Netty  |

> WebFlux是基于响应式流的，可以用来建立异步、非阻塞、事件驱动的服务。默认采用Reactor作为响应式流的实现库

## 3、限流

### 3.1、简介

在高并发的系统中，往往需要在系统中做限流，一方面是为了**防止大量的请求使服务器过载**，导致服务不可用，另一方面是为了**防止网络攻击**。

一般开发高并发系统常见的限流有：

- 限制总并发数（比如数据库连接池、线程池）
- 限制瞬时并发数（如 nginx 的 limit_conn 模块，用来限制**瞬时并发连接数**）
- 限制时间范围内的平均速率（如 Guava 的 RateLimiter、nginx 的 limit_req 模块，**限制每秒的平均速率**）
- 其他还有如限制远程接口调用速率、限制 MQ 的消费速率
- 另外还可以根据网络连接数、网络流量、CPU 或内存负载等来限流。

### 3.2、算法

简单的做法是维护一个单位时间内的 计数器，每次请求计数器加1，当单位时间内计数器累加到大于设定的阈值，则之后的请求都被拒绝，直到单位时间已经过去，再将 计数器 重置为零。此方式有个弊端：如果在单位时间1s内允许100个请求，在10ms已经通过了100个请求，那后面的990ms，只能眼巴巴的把请求拒绝，我们把这种现象称为“突刺现象”。

常用的更平滑的限流算法有两种：**漏桶算法** 和 **令牌桶算法**。

#### 3.2.1、漏桶算法

漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里，漏桶以一定的速度出水(每秒响应速 率)，当水流入速度过大会直接溢出(访问频率超过接口响应速率)，然后就拒绝请求，可以看出漏桶算法 能强行限制数据的传输速率。示意图如下：

![1591348926532](就业技术加强-07-微服务扩展-1.assets/1591348926532.png) 

这里有两个变量，一个是**桶的大小**，支持流量突发增多时可以存多少的水（burst），另一个是水桶**漏洞的大小**（rate）。漏桶算法能够强行**限制数据的传输速率**。因为漏桶的漏出速率是固定的参数，所以，即使网络中不存在资源冲突（没有发生拥塞），漏桶算法也不能使流突发（burst）到端口速率；因此，漏桶算法对于存在突发特性的流量来说缺乏效率。

#### 3.2.2、令牌桶算法

令牌桶算法 和 漏桶算法 效果一样但方向相反的算法，更加容易理解。随着时间流逝，系统会按恒定 1/QPS 时间间隔（如果 QPS=100，则间隔是 10ms）往桶里加入 Token令牌，如果桶已经满了就不再加了。新请求来临时，会各自拿走一个 Token，如果没有 Token 可拿了就阻塞或者拒绝服务。

![1591349193863](就业技术加强-07-微服务扩展-1.assets/1591349193863.png) 

令牌桶算法能够在限制数据的平均传输速率的同时还允许某种程度的**突发传输**。一旦需要提高速率，则按需提高放入桶中的令牌的速率。一般会定时（比如 100 毫秒）往桶中增加一定数量的令牌，有些变种算法则实时的计算应该增加的令牌的数量。

### 3.3、Nginx限流

#### 3.3.1、控制单个IP速率

**1）配置访问速率**

控制速率的方式采用漏桶算法；在Nginx中通过 ` limit_req_zone key zone rate`  实现；具体意义：

```
key: 定义限流对象，binary_remote_addr是一种key，表示基于remote_addr(客户端IP)来做限流，binary_的目的是压缩内存占用量。

zone: 定义共享内存区来存储访问信息，myLimit:10m 表示一个大小为10M，名字为myLimit的内存区域。1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息。

rate: 用于设置最大访问速率，rate=2r/s 表示每秒最多处理2个请求。Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 2r/s 实际上是限制：每500毫秒处理一个请求。
```

1）解压 `资料\nginx-1.19.1.zip` 

2）配置 `nginx-1.19.1\conf\nginx.conf` 文件；（可删除大部分注释内容）

- ①大概在第 34 行，添加如下：

```properties
#定义内存区myLimit大小为10m，限制同一个ip，每秒钟最多处理2个请求
limit_req_zone $binary_remote_addr zone=myLimit:10m rate=2r/s;
```

![image-20210107214221562](就业技术加强-07-微服务扩展-1.assets/image-20210107214221562.png)

- ②在大概第 45 行添加内容如下；

```conf
location / {
    limit_req zone=myLimit;
    proxy_pass http://127.0.0.1:10086;
}
```

![image-20210107214325521](就业技术加强-07-微服务扩展-1.assets/image-20210107214325521.png)

3）启动Nginx（ `start .\nginx.exe` ）；访问 http://localhost/user/1 测试；

![image-20210107213427866](就业技术加强-07-微服务扩展-1.assets/image-20210107213427866.png)

如果上述地址在1秒内超过2次的话：

![image-20210107214354148](就业技术加强-07-微服务扩展-1.assets/image-20210107214354148.png)



**2）处理突发流量（排队）**

上面例子限制 2r/s，如果流量突然增大，超出的请求将被拒绝，无法处理突发流量；可以通过在上述的配置中结合 burst 参数使用来解决该问题。

![image-20210108105058936](就业技术加强-07-微服务扩展-1.assets/image-20210108105058936.png)

>burst 译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数,当rate=2r/s 时，将1s拆成2份，即每500ms可处理1个请求。 
>
>此处，burst=4，若1秒内同时有6个请求到达，Nginx会处理按速率1秒内处理2个请求，剩余4个请求将放入队列，然后每隔500ms从队列中获取一个请求进行处理。若1秒内请求数大于6，将拒绝处理多余的请求，直接返回503. 
>
>不过，单独使用burst参数并不实用。假设burst=50，rate依然为10r/s，排队中的50个请求虽然每100ms会处理一个，但第50个请求却需要等待50*100ms即5s，这么长的处理时间自然难以接受。

![image-20210108111035122](就业技术加强-07-微服务扩展-1.assets/image-20210108111035122.png)

![image-20210108111048344](就业技术加强-07-微服务扩展-1.assets/image-20210108111048344.png)

![image-20210108111054878](就业技术加强-07-微服务扩展-1.assets/image-20210108111054878.png)

3）**处理突发流量（不排队）**

修改配置文件；平均每秒允许不超过2个请求，突发不超过4个请求，并且处理突发4个请求的时候，没有延迟，等到完成之后，按照正常的速率处理。

![image-20210108111522535](就业技术加强-07-微服务扩展-1.assets/image-20210108111522535.png)

重启nginx（ `.\nginx.exe -s reload` ），可测试1秒内并发7个请求处理结果如下：

![image-20210108111619335](就业技术加强-07-微服务扩展-1.assets/image-20210108111619335.png)

#### 3.3.2、控制并发量

在nginx中的配置文件中 ngx_http_limit_conn_module 提供了限制连接数的能力。主要是利用limit_conn_zone和limit_conn两个指令。 

利用连接数限制 某一个用户的ip连接的数量来控制流量。 

> 注意：并非所有连接都被计算在内 只有当服务器正在处理请求并且已经读取了整个请求头时，才会计算有效连接。此处忽略测试。

- **限制单个IP连接数**

1）修改 `nginx.conf` 文件，在大概第 36 行，添加如下：

```properties
# 根据IP地址来限制连接数，存储内存大小10M 
limit_conn_zone $binary_remote_addr zone=addr:10m;
```

![image-20210108112839058](就业技术加强-07-微服务扩展-1.assets/image-20210108112839058.png)

2）修改 `nginx.conf` 文件，在大概第 42 行，修改如下：

```
location / {
	# limit_req zone=myLimit burst=4 nodelay;
	# 配置单个客户端ip与服务器的连接数为2
	limit_conn addr 2;
	proxy_pass http://127.0.0.1:10086;
}
```

![image-20210108112846831](就业技术加强-07-微服务扩展-1.assets/image-20210108112846831.png)

3）修改 `UserController` 

![image-20210108112905352](就业技术加强-07-微服务扩展-1.assets/image-20210108112905352.png)

4）测试

![image-20210108112947626](就业技术加强-07-微服务扩展-1.assets/image-20210108112947626.png)



- **限制服务器最大连接数**

上面的限制了单个IP的最大连接数；在nginx中也可以通过如下配置设置连接的服务器的最大连接数。

1）修改 `nginx.conf` 文件，在大概第 36 行，修改如下：

```properties
# 根据服务器提供的总连接数做限制，存储内存大小10M 
limit_conn_zone $server_name zone=perserver:10m;

```

![image-20210108153010138](就业技术加强-07-微服务扩展-1.assets/image-20210108153010138.png)

2）修改 `nginx.conf` 文件，在大概第 42 行，修改如下：

```properties
# 限制与服务器的总连接数
limit_conn perserver 10;
```

![image-20210108153018658](就业技术加强-07-微服务扩展-1.assets/image-20210108153018658.png)

3）测试

![image-20210108153137579](就业技术加强-07-微服务扩展-1.assets/image-20210108153137579.png)

### 3.4、Gateway限流

Spring Cloud Gateway 可以配置限流策略对请求进行限流；默认使用redis的RateLimiter令牌桶限流算法来实现。

![image-20191213095741252](就业技术加强-07-微服务扩展-1.assets/image-20191213095741252.png)

如果要配置的话；则在网关所在工程中配置如下：

1）添加依赖；

```xml
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-redis-reactive</artifactId>
        </dependency>
```

2）配置限流键解析器；

```java
@SpringBootApplication
@EnableDiscoveryClient
public class GatewayApplication {
    public static void main(String[] args) {
        SpringApplication.run(GatewayApplication.class, args);
    }

    /**
     * 根据ip限流
     * @return ip信息
     */
    @Bean
    @Primary
    public KeyResolver ipKeyResolver(){
        return new KeyResolver() {
            @Override
            public Mono<String> resolve(ServerWebExchange exchange) {
                return Mono.just(exchange.getRequest().getRemoteAddress().getAddress().getHostName());
            }
        };
    }

    /**
     * 根据uri限流
     * @return uri
     */
    @Bean
    public KeyResolver uriKeyResolver(){
        return new KeyResolver() {
            @Override
            public Mono<String> resolve(ServerWebExchange exchange) {
                return Mono.just(exchange.getRequest().getPath().value());
            }
        };
    }
}

```

> KeyResolver用于计算某一个类型的限流的KEY也就是说，可以通过KeyResolver来指定限流的Key。
>
> 我们可以根据IP来限流，比如每个IP每秒钟只能请求一次，在`GatewayApplication`定义key的获取，获取客户端IP，将IP作为key。
>
> Mono.just表示的是包含 0 或者 1 个元素的异步序列；请求并等待。如：Mono<Void>用于在异步任务完成时发出通知
>
> 另外；hostAddress在本机本地访问会输出 0:0:0:0:0:0:0:1 ，非本地则不会如此。

3）配置限流令牌桶策略及redis信息

```yml
server:
  port: 10086
spring:
  application:
    name: heima-gateway
  cloud:
    gateway:
      routes:
        - id: user-route
          uri: lb://heima-user
          predicates:
            - Path=/user/**
          filters:
            - name: RequestRateLimiter #请求数限流
              args:
                #限流键解析器
                key-resolver: "#{@ipKeyResolver}"
                # 令牌产生速率
                redis-rate-limiter.replenishRate: 1
                # 令牌桶突发容量
                redis-rate-limiter.burstCapacity: 1
  redis:
    host: 127.0.0.1
    port: 6379
eureka:
  client:
    service-url:
      defaultZone: http://127.0.0.1:9090/eureka
  instance:
    ip-address: 127.0.0.1
    prefer-ip-address: true
```

4）测试

![image-20210108174628985](就业技术加强-07-微服务扩展-1.assets/image-20210108174628985.png)

![image-20210108174505960](就业技术加强-07-微服务扩展-1.assets/image-20210108174505960.png)

发出12个请求；10个失败，2个成功。常规1秒1个令牌加上突发令牌桶容量1共2个请求可被处理。

## 4、分布式日志GrayLog

### 4.1、分布式日志

在微服务架构下，微服务被拆分成多个微小的服务，每个微服务都部署在不同的服务实例上，当我们定位问题，检索日志的时候需要依次登录每台服务器进行检索。

这样是不是感觉很繁琐和效率低下？所以我们还需要一个工具来帮助集中收集、存储和检索这些跟踪信息。

集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事。以前，我们通过使用grep、awk、和wc等linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。



分布式日志服务就是来帮我们解决上述问题的。其基本思路是:

- 日志收集器: 微服务中引入日志客户端，将记录的日志发送到日志服务端的收集器，然后以某种方式存储
- 日志数据存储: 一般使用ElasticSearch分布式存储，把收集器收集到的日志格式化，然后存储到分布式存储中
- 日志查看: 利用ElasticSearch的统计搜索功能，实现日志查询和报表输出

比较知名的分布式日志服务包括:

- ELK：ElasticSearch、Logstash、Kibana
- GrayLog

### 4.2、GrayLog简介

业界比较知名的分布式日志服务解决方案是ELK，而我们今天要学习的是GrayLog。为什么呢？

ELK解决方案问题:

+ 不能处理多行日志，比如mysql慢查询，Tomcat、Jetty应用的Java异常打印

+ 不能保留原始日志，只能把原始日志分字段保存，这样搜索日志结果是一堆Json格式文本，无法阅读。

+ 不符合正则表达式匹配的日志行，被全部丢弃。

  

GrayLog方案优势:

+ 一体化方案，安装方便，不像ELK有3个独立系统间的集成问题。

+ 采集原始日志，并可以事后再添加字段，比如http_status_code，response_time等等

+ 自己开发采集日志脚本，并用curl/nc发送到GrayLog Server，发送格式是自定义的GELF，Flunted和Logstash都有相应的输出GELF消息的插件。自己开发带来很大的自由度。实际上只需要用inotifywait监控日志的modify事件，并把日志的新增行用curl/netcat发送到GrayLog Server即可。

+ 搜索结果高亮显示，就像google一样。

+ 搜索语法简单，比如:`source:mongo AND reponse_time_ms:>5000`避免直接输入ElasticSearch搜索的json语法。

+ 搜索条件可以导出为ElasticSearch的搜索json脚本，方便直接开发调用ElasticSearch rest api的搜索脚本。


官网: https://www.graylog.org

 ![image-20210108175717041](就业技术加强-07-微服务扩展-1.assets/image-20210108175717041.png)

基本框架图:

![1591426416655](就业技术加强-07-微服务扩展-1.assets/1591426416655.png)

### 4.3、GayLog安装

#### 4.3.1、环境准备

- centos7

- docker

- jdk1.8

- ElasticSearch6.x

- mongodb

  

#### 4.3.2、安装ElasticSearch

注意: GrayLog3 不支持ElasticSearch7.x版本，必须用6的版本。

```shell
# 拉取镜像
docker pull elasticsearch:6.6.2

# 创建容器
docker run -d --privileged --name=graylog-es \
-e "http.host=0.0.0.0" \
-e "network.host=0.0.0.0" \
-e "TAKE_FILE_OWNERSHIP=true" \
-e "ES_JAVA_OPTS=-Xms512m -Xmx512m" \
-e "discovery.type=single-node" \
-v es-data:/usr/share/elasticsearch/data \
-v es-logs:/usr/share/elasticsearch/logs \
-p 9201:9200 \
-p 9301:9300 \
elasticsearch:6.6.2
```

可访问 http://192.168.12.135:9201/ 测试。



#### 4.3.3、安装MongoDB

```shell
# 拉取镜像
docker pull mongo

# 创建容器
docker run -d --privileged --name=graylog-mongo \
-v mongo-configdb:/data/configdb/ \
-v mongo-db:/data/db/ \
-p 27017:27017 \
mongo
```



#### 4.3.4、安装GrayLog

```shell
# 拉取镜像
docker pull graylog/graylog:3.3.0

# 创建容器 --link: 指定链接的容器(容器通信)
docker run -d --name=graylog \
--link graylog-mongo:mongo \
-p 9000:9000 \
-p 12201:12201/udp \
-e GRAYLOG_HTTP_EXTERNAL_URI=http://192.168.12.135:9000/ \
-e GRAYLOG_ELASTICSEARCH_HOSTS=http://192.168.12.135:9201/ \
-e GRAYLOG_ROOT_TIMEZONE="Asia/Shanghai" \
-e GRAYLOG_WEB_ENDPOINT_URI="http://192.168.12.135:9000/:9000/api" \
-e GRAYLOG_ROOT_PASSWORD_SHA2=8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918 \
graylog/graylog:3.3.0
```



#### 4.3.5、访问GrayLog

访问：http://192.168.12.135:9000

![image-20210108203217473](就业技术加强-07-微服务扩展-1.assets/image-20210108203217473.png)

输入用户名与密码，都是`admin`，进入首页，因为没有数据，暂看不到日志信息：

![image-20210108203726840](就业技术加强-07-微服务扩展-1.assets/image-20210108203726840.png)

![image-20210108203840561](就业技术加强-07-微服务扩展-1.assets/image-20210108203840561.png)

#### 4.3.6、配置GrayLog

配置日志采集方式

![image-20210108204112117](就业技术加强-07-微服务扩展-1.assets/image-20210108204112117.png)

![image-20210108204504502](就业技术加强-07-微服务扩展-1.assets/image-20210108204504502.png)

![image-20210108204512235](就业技术加强-07-微服务扩展-1.assets/image-20210108204512235.png)

配置完后如下：

![image-20210108204520185](就业技术加强-07-微服务扩展-1.assets/image-20210108204520185.png)

GrayLog配置完后，需要到我们的项目中设置日志信息。

### 4.4、GrayLog应用

GrayLog的服务端日志收集器已经准备好，我们还需要在项目中添加GrayLog的客户端，将项目日志发送到GrayLog服务端，保存到ElasticSearch。分布式日志服务都是无侵入的，所以只需要配置即可。

#### 4.4.1、添加依赖

这个是第三方提供的GrayLog依赖，并不是GrayLog官网。

将下方的依赖添加到有需要使用GrayLog的项目；如果都需要那么则都需要加，包括之后的配置。

```xml
<dependency>
    <groupId>biz.paluch.logging</groupId>
    <artifactId>logstash-gelf</artifactId>
    <version>1.13.0</version>
</dependency>
```



#### 4.4.2、日志配置文件

在需要添加日志的每个项目的`resource`目录下添加`logback.xml`文件:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- 控制台日志输出器 -->
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <!-- 对日志进行格式化 -->
        <!--格式化输出：%d表示日期，%thread线程名，%-5level：级别 %msg：日志消息，%n是换行符-->
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- GrayLog日志输出器 -->
    <appender name="GELF" class="biz.paluch.logging.gelf.logback.GelfLogbackAppender">
        <!-- GrayLog服务端连接主机 【重要】-->
        <host>udp:192.168.12.135</host>
        <!-- GrayLog服务端连接端口 【重要】-->
        <port>12201</port>
        <!-- GrayLog服务端版本号 -->
        <version>1.1</version>
        <!-- 当前服务名称(服务id) 【重要】-->
        <facility>heima-gateway</facility>
        <!-- 附加字段 -->
        <additionalFields>version=4.1.0,module=heima-gateway</additionalFields>
        <!-- 记录异常跟踪栈日志 -->
        <extractStackTrace>true</extractStackTrace>
        <!-- 过滤异常跟踪栈日志 -->
        <filterStackTrace>true</filterStackTrace>
        <!-- 映射分析搜集的日志 -->
        <mdcProfiling>true</mdcProfiling>
        <!-- 时间戳格式化 -->
        <timestampPattern>yyyy-MM-dd HH:mm:ss,SSS</timestampPattern>
        <!-- 最大消息大小 -->
        <maximumMessageSize>8192</maximumMessageSize>
    </appender>

    <!-- 记录日志 additivity: false(只能在当前logger中使用) -->
    <logger name="com.bhd" level="DEBUG" additivity="false">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="GELF"/>
    </logger>

    <!-- 根记录日志 -->
    <root level="INFO">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="GELF"/>
    </root>
</configuration>
```

```properties
日志级别: 有8个级别的log（除去OFF和ALL，可以说分为6个级别），优先级从高到低依次为：OFF、FATAL、ERROR、WARN、INFO、DEBUG、TRACE、 ALL。

级别大小:
ALL > TRACE > DEBUG > INFO > WARN > ERROR > FATAL > OFF
```

配置完后；重启项目。



#### 4.4.3、查看日志

![image-20210108211647270](就业技术加强-07-微服务扩展-1.assets/image-20210108211647270.png)

![image-20210108211714564](就业技术加强-07-微服务扩展-1.assets/image-20210108211714564.png)

编辑面板：

![image-20210108212152072](就业技术加强-07-微服务扩展-1.assets/image-20210108212152072.png)

![image-20210108212239422](就业技术加强-07-微服务扩展-1.assets/image-20210108212239422.png)



保存查看视图：

![image-20210108212322653](就业技术加强-07-微服务扩展-1.assets/image-20210108212322653.png)



创建面板：

![image-20210108212336281](就业技术加强-07-微服务扩展-1.assets/image-20210108212336281.png)

![image-20210108212343256](就业技术加强-07-微服务扩展-1.assets/image-20210108212343256.png)

![image-20210108212350129](就业技术加强-07-微服务扩展-1.assets/image-20210108212350129.png)

查看详细信息：

![image-20210108213526566](就业技术加强-07-微服务扩展-1.assets/image-20210108213526566.png)

![image-20210108213532425](就业技术加强-07-微服务扩展-1.assets/image-20210108213532425.png)

![image-20210108213540515](就业技术加强-07-微服务扩展-1.assets/image-20210108213540515.png)



上述这些面板的数据更新也可以通过设置如下进行定时更新：

![image-20210108213719936](就业技术加强-07-微服务扩展-1.assets/image-20210108213719936.png)

## 5、分布式链路追踪Sleuth

### 5.1、分布式链路追踪

#### 5.1.1、现况

随着业务的发展，单体架构演变成微服务架构，并且系统规模也变得越来越大，各个服务之间的调用关系也变得越来越复杂。在微服务的应用中，一个由客户端发起的请求到请求结束中间可能会有多个不同的服务调用，然后产生最终的结果。**调用链路容易出错**；客户端发起请求都会形成一个复杂的分布式服务调用链路，在每条链路中任何一个服务出现延迟或者超时或者异常都有可能引起整个请求异常。

![1591414864164](就业技术加强-07-微服务扩展-1.assets/1591414864164-1592205999192.png)



#### 5.1.2、链路追踪

**微服务架构**是一个分布式架构，它按业务划分服务单元，一个分布式系统往往有很多个服务单元。由于服务单元数量众多，业务的复杂性，如果**出现了错误和异常，很难去定位**。主要体现在，一个请求可能需要调用很多个服务，而内部服务的调用复杂性，决定了问题难以定位。所以微服务架构中，**必须实现分布式链路追踪**，去跟进一个请求到底**有哪些服务参与，参与的顺序又是怎样的，从而达到每个请求的步骤清晰可见，出了问题，很快定位**。

链路追踪组件有Google的Dapper，Twitter 的Zipkin，Apache的Skywalking，以及阿里的Eagleeye （鹰眼）等，它们都是非常优秀的链路追踪开源组件。而Spring Cloud Sleuth可以很方便的集成Zipkin、Skywalking等这些组件；这些组件也被称为 APM 系统（Application Performance Management）**应用性能管理系统**。

> Google开源的 Dapper链路追踪组件，并在2010年发表了论文《Dapper, a Large-Scale Distributed  Systems Tracing Infrastructure》，这篇文章是业内实现链路追踪的标杆和理论基础，具有非常大的参考价值。

### 5.2、Spring Cloud Sleuth

#### 5.2.1、简介

Spring Cloud Sleuth是Spring Cloud提供的分布式系统服务链路追踪组件。Spring Cloud Sleuth是一个在应用中实现日志跟踪的强有力的工具。它在整个分布式系统中能跟踪一个用户请求的过程(包括数据采集，数据传输，数据存储，数据分析，数据可视化)，捕获这些跟踪数据，就能构建微服务的整个调用链的视图，这是调试和监控微服务的关键工具。

| 特点             | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| 提供链路追踪     | 通过sleuth可以很清楚的看出一个请求经过了哪些服务， 可以方便的理清服务间的调用关系 |
| 性能分析         | 通过sleuth可以很方便的看出每个采集请求的耗时， 分析出哪些服务调用比较耗时，当服务调用的耗时随着请求量的增大而增大时， 也可以对服务的扩容提供一定的提醒作用 |
| 数据分析优化链路 | 对于频繁地调用一个服务，或者并行地调用等， 可以针对业务做一些优化措施 |
| 可视化           | 对于程序未捕获的异常，可以在zipkin、skywalking等系统上的界面看到 |

#### 5.2.2、专业术语

![image-20210109164813816](就业技术加强-07-微服务扩展-1.assets/image-20210109164813816.png)

- **Trace**：它是由一组有相同Trace  ID的Span串联形成一个树状结构。为了实现请求跟踪，当请求到分布式系统的入口端点时，只需要服务跟踪框架为该请求创建一个唯一的跟踪标识（即前文提到的Trace ID），同时在分布式系统内部流转的时候，框架始终保持传递该唯一标识，直到返回请求为止，我们通过它将所有请求过程中的日志关联起来；

- **Span**：它代表了一个基础的工作单元，例如服务调用。为了统计各处理单元的时间延迟，当前请求到达各个服务组件时，也通过一个唯一标识（即前文提到的Span  ID）来标记它的开始、具体过程以及结束。通过span的开始和结束的时间戳，就能统计该span的时间延迟，除此之外，我们还可以获取如事件名称、请求信息等元数据。

- **Annotation**：它用于记录一段时间内的事件。内部使用的最重要的注释是：

  - cs - Client Sent - 客户端发送一个请求，这个注解描述了这个Span的开始。

  - sr - Server Received - 服务端获得请求并准备开始处理它，其中（sr – cs） 时间戳便可得到网络传输的时间。

  - ss - Server Sent （服务端发送响应）– 该注解表明请求处理的完成(当请求返回客户端)， （ss – sr）时间戳就可以得到服务器请求的时间。

  - cr - Client Received （客户端接收响应）- 表明此时Span的结束，（cr – cs）时间戳便可以得到整个请求所消耗的时间。

- **采样率**：对多少请求进行采用统计、分析

如果服务的流量很大，全部采集对传输、存储压力比较大。这个时候可以设置采样率，sleuth 可以通过配置  spring.sleuth.sampler.probability=X.Y（如配置为1.0，则采样率为100%，采集服务的全部追踪数据），若不配置默认采样率是0.1(即10%)。

#### 5.2.3、应用

Sleuth可以直接使用；只是这样的话它只能将追踪信息输出到控制台，一般很少直接使用sleuth；但是如果使用的话也是非常简单，只需要有使用的项目中添加如下的依赖到 `pom.xml` 即可。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-sleuth</artifactId>
</dependency>
```

之后启动系统的话；可以在控制台看到类似如下的信息：

![image-20210109165708758](就业技术加强-07-微服务扩展-1.assets/image-20210109165708758.png)

> 如果在控制台中没有输出类似上述的追踪链路信息；可能是你的项目中的日志记录信息没有设置上述的信息输出。解决方法：将对应日志配置文件重命名或者直接删除即可。如: logback.xml 可以先改名为 logback.xml.bak之后查看上述的输出效果。

### 5.3、Zipkin

#### 5.3.1、简介

Zipkin是Twitter的一个开源项目，是一个分布式跟踪系统。它有助于收集解决服务体系结构中的延迟问题所需的时序数据。功能包括该数据的收集和查找。 

Zipkin UI还提供了一个依赖关系图，该关系图显示了每个应用程序中跟踪了多少个请求。这对于识别聚合行为（包括错误路径或对不赞成使用的服务的调用）很有帮助。 

与Sleuth集成后，可以通过Zipkin的界面更加优化的查看链路信息、数据分析等。

官网地址: <https://zipkin.io/> 

Zipkin分为 Server服务器端、Client客户端：

-  Server服务器端：包含四个组件，分别是collector、storage、search、web UI

   - **collector 信息收集器**，作为一个守护进程，它会时刻等待客户端传递过来的追踪数据，对这些数据进行验证、存储以及创建查询需要的索引。

   - **storage 存储组件**，zipkin默认直接将数据存在内存中，此外支持使用Cassandra、ElasticSearch 和 Mysql。

   - **search 查询搜索**，它提供了简单的RESTful API来供外部调用查询。

   - **web UI** 是zipkin的服务端展示平台，主要调用search提供的接口，用图表将链路信息清晰地展示给开发人员。

-  Client客户端：主要负责根据应用的调用情况生成追踪信息，并且将这些追踪信息发送至zipkin由收集器接收。**其实就是我们的项目微服务**添加了zipkin的客户端依赖包。

**Zipkin工作流程图**：

![image-20210109171025290](就业技术加强-07-微服务扩展-1.assets/image-20210109171025290.png)

#### 5.3.2、安装

zipkin可以下载[独立](https://github.com/openzipkin/zipkin)的服务器端直接 `java -jar`运行，也可以以docker容器启动；以下以docker安装方式说明：

+ docker安装zipkin-server

  ```shell
  # 拉到镜像 
  docker pull openzipkin/zipkin
  
  # 创建容器
  docker run -d -p 9411:9411 --name=zipkin openzipkin/zipkin
  ```

+ 访问: http://192.168.12.135:9411

![image-20210109172153368](就业技术加强-07-微服务扩展-1.assets/image-20210109172153368.png)

#### 5.3.3、应用

在所有微服务工程中可以配置如下：

1）添加依赖

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-zipkin</artifactId>
</dependency>
```

2）添加配置信息

```yml
spring:
  zipkin:
    base-url: http://192.168.12.135:9411/
  sleuth:
    sampler:
      probability: 1.0 
```

> 系统不多，请求也少；所以设置采样率probability为 1.0；默认为 0.1

3）重启服务器，访问服务 http://localhost:10086/user/1 

4）访问Zipkin http://192.168.12.135:9411

![image-20210109173939878](就业技术加强-07-微服务扩展-1.assets/image-20210109173939878.png)

![image-20210109173948761](就业技术加强-07-微服务扩展-1.assets/image-20210109173948761.png)

### 5.4、Skywalking

#### 5.4.1、简介

Skywalking与Zipkin同样是属于一款分布式链路追踪系统。Skywalking是由国内开源并提交到Apache孵化器的产品，它同时吸收了Zipkin/Pinpoint/CAT的设计思路，支持非侵入式埋点。是一款基于分布式跟踪的应用程序性能监控系统。被用于追踪、监控和诊断分布式系统，特别是使用微服务架构，云原生或容器技术。官网地址： http://skywalking.apache.org

**提供以下主要功能**：

- 分布式追踪和上下文传输
- 应用、实例、服务性能指标分析
- 应用拓扑分析
- 应用和服务依赖分析
- 慢服务检测
- 性能优化

**Skywalking与Zipkin区别**：

- **颗粒度**：Skywalking方法级（展示的更详细），方法中所有的调用都展示出来了，如数据库调用、redis调用，第三方网络调用，而Zipkin只能展示接口级
- **UI界面**：Skywalking完胜，国产开源，更适合国人眼球
- **代码侵入性**：Skywalking无代码侵入，使用字节码增强技术，在启动服务时使用 javaagent 指向skywalking服务即可收集调用链span信息
- **Zipkin**：简单、轻量级

#### 5.4.2、安装

**1）上传与解压**

```shell
cd /usr/local

# 使用工具将 资料\apache-skywalking-apm-6.5.0.tar.gz 文件上传到 /usr/local

# 解压
tar -xzvf apache-skywalking-apm-6.5.0.tar.gz
```

**2）配置存储组件为ElasticSearch**

修改存储组件为 ElasticSearch ；进入config目录，修改 application.yml，主要把存储方案从h2改为ElasticSearch

```shell
cd /usr/local/apache-skywalking-apm-bin/config

编辑 application.yml；具体查看如下图，修改前后对比：
```

修改前：

![image-20210109180313245](就业技术加强-07-微服务扩展-1.assets/image-20210109180313245.png)



修改后：

![image-20210109180520316](就业技术加强-07-微服务扩展-1.assets/image-20210109180520316.png)

**3）启动**

启动前请检查 `ElasticSearch` 是启动的。

```shell
/usr/local/apache-skywalking-apm-bin/bin/startup.sh
```

访问：http://192.168.12.135:8080

![image-20210109180844830](就业技术加强-07-微服务扩展-1.assets/image-20210109180844830.png)

#### 5.4.3、应用

**1）解压并获取探针包**

使用一般的解压工具解压 `资料\apache-skywalking-apm-6.5.0.tar.gz` 

![image-20210109181120747](就业技术加强-07-微服务扩展-1.assets/image-20210109181120747.png)

**2）配置项目启动项**

Skywalking是无侵入性的，在开发阶段，不用改代码，只需要在部署启动时加入一些参数即可。在IDEA中的项目启动项中设置 VM 参数如下：

```shell
-javaagent:F:/tmp/workspaces/apache-skywalking-apm-bin/agent/skywalking-agent.jar
-Dskywalking.agent.service_name=heima-eureka
-Dskywalking.collector.backend_service=192.168.12.135:11800
```

> - `-javaagent:F:/tmp/workspaces/apache-skywalking-apm-bin/agent/skywalking-agent.jar`：配置的是skywalking-agent.jar这个包的位置，要修改成你自己存放的目录
>
> - `-Dskywalking.agent.service_name=heima-eureka`：是当前项目的名称，需要改成当前微服务的名称
>
> - `-Dskywalking.collector.backend_service=192.168.12.135:11800`：是skywalking的OPA服务地址，采用的是GRPC通信，因此端口是11800，不是8080

具体配置步骤如下：

![image-20210109181703926](就业技术加强-07-微服务扩展-1.assets/image-20210109181703926.png)

![image-20210109181839134](就业技术加强-07-微服务扩展-1.assets/image-20210109181839134.png)



其它项目如上图类似配置即可。配置完后启动项目。

**3）测试**

访问服务 http://localhost:10086/user/1 

访问Skywalking http://192.168.12.135:8080/

![image-20210109182444962](就业技术加强-07-微服务扩展-1.assets/image-20210109182444962.png)

![image-20210109182519546](就业技术加强-07-微服务扩展-1.assets/image-20210109182519546.png)

